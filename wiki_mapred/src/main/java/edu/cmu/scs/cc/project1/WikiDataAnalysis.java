package edu.cmu.scs.cc.project1;

/**
 * MapReduce job workflow in hadoop for wiki data analysis.
 *
 * <p>TODO: Implement a MapReduce application in hadoop.
 *
 * <p>Hint: you could refer to the code in word count task.
 *
 * <p>All the steps below are described in a way aligned with Javadoc,
 * if you are unsure which methods to use, read the Javadoc of
 * {@link org.apache.hadoop.mapreduce.Job}.
 *
 * <p>1. Set the key/value class for the map output data.
 * 2. Set the key/value class for the job output data.
 * 3. Set the Mapper and Reducer class for the job.
 * 4. Set the number of Reducer tasks.
 * 5. Submit the job to the cluster and not wait for it to finish, i.e.
 * {@code job.waitForCompletion(true);}
 * 6. return the job, i.e.
 */
public class WikiDataAnalysis {

    public static void main(String[] args) {


    }

}
